# -*- coding: utf-8 -*-
"""resume.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZB-5N0z3pdLz0DganSJaCZBPecz9nYxB
"""

# Installing all required libraries

#import the libraries
import fitz
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-MiniLM-L6-v2')

# Pre Process Text
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')


# Extracting text from Resume

def extract_text_from_resume(pdf_file):
  doc = fitz.open(stream = pdf_file.read(), filetype = "pdf")
  text_in_resume = ""
  for page in doc:
    text_in_resume = text_in_resume + page.get_text()
  return text_in_resume


def preprocess_text(text):
  text = re.sub(r'\W+', ' ', text.lower())
  words = text.split()
  lemmatizer = WordNetLemmatizer()
  words_after_stopwords = []
  for w in words:
    if w not in stopwords.words('english'):
      lemma = lemmatizer.lemmatize(w)
      words_after_stopwords.append(lemma)
  return ' '.join(words_after_stopwords)


def compute_similarity(resume_text, jd):
  cleaned_resume = preprocess_text(resume_text)
  cleaned_jd = preprocess_text(jd)

  resume_embedding  = model.encode(cleaned_resume, convert_to_tensor = True)
  jd_embedding  = model.encode(cleaned_jd, convert_to_tensor = True)

  similarity_scores = util.cos_sim(resume_embedding, jd_embedding)[0][0].item()
  return similarity_scores




def missing_keywords(resume_text, jd_text):
  resume_tokens = set( preprocess_text(resume_text).split())
  jd_tokens = set(preprocess_text(jd_text).split())
  missing = jd_tokens - resume_tokens
  return list(missing)[:5]
